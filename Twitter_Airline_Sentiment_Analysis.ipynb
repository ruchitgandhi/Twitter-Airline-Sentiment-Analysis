{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gemin\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py:313: FutureWarning: numpy not_equal will not check object identity in the future. The comparison did not return the same result as suggested by the identity (`is`)) and will change.\n",
      "  _nan_object_mask = _nan_object_array != _nan_object_array\n",
      "C:\\Users\\gemin\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import tree, model_selection\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import io\n",
    "import datetime as dt\n",
    "import string\n",
    "import unicodedata\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "tokenizer = ToktokTokenizer()\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "from gensim import corpora\n",
    "\n",
    "%pylab --no-import-all inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_id                            0\n",
       "airline_sentiment                   0\n",
       "airline_sentiment_confidence        0\n",
       "negativereason                   5462\n",
       "negativereason_confidence        4118\n",
       "airline                             0\n",
       "airline_sentiment_gold          14600\n",
       "name                                0\n",
       "negativereason_gold             14608\n",
       "retweet_count                       0\n",
       "text                                0\n",
       "tweet_coord                     13621\n",
       "tweet_created                       0\n",
       "tweet_location                   4733\n",
       "user_timezone                    4820\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find NaN values in each column\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_id                           0\n",
       "airline_sentiment                  0\n",
       "airline_sentiment_confidence       0\n",
       "negativereason                     0\n",
       "negativereason_confidence          0\n",
       "airline                            0\n",
       "airline_sentiment_gold          9146\n",
       "name                               0\n",
       "negativereason_gold             9146\n",
       "retweet_count                      0\n",
       "text                               0\n",
       "tweet_coord                     8515\n",
       "tweet_created                      0\n",
       "tweet_location                  3142\n",
       "user_timezone                   3170\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Are there tweets with negative label but no negative reason?\n",
    "# This would help us in finding if negative label is a unique identifier for negative tweets or not.\n",
    "neg_df = df.loc[df['airline_sentiment']=='negative']\n",
    "neg_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5462\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tweet_id                           0\n",
       "airline_sentiment                  0\n",
       "airline_sentiment_confidence       0\n",
       "negativereason                  5462\n",
       "negativereason_confidence       4118\n",
       "airline                            0\n",
       "airline_sentiment_gold          5454\n",
       "name                               0\n",
       "negativereason_gold             5462\n",
       "retweet_count                      0\n",
       "text                               0\n",
       "tweet_coord                     5106\n",
       "tweet_created                      0\n",
       "tweet_location                  1591\n",
       "user_timezone                   1650\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Are there tweets with non negative label but negative reason\n",
    "pos_df = df.loc[df['airline_sentiment']!='negative']\n",
    "print(len(pos_df))\n",
    "pos_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    0.626913\n",
       "neutral     0.211680\n",
       "positive    0.161407\n",
       "Name: airline_sentiment, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Class Distribution or prediction variable\n",
    "df['airline_sentiment'].value_counts()/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations are: 14640\n"
     ]
    }
   ],
   "source": [
    "#remove any rows that has no tweet text\n",
    "dataset=df.text.dropna()\n",
    "print('Number of observations are: '+str(len(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet_length'] = df['text'].apply(lambda x : len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_hashtags'] = df['text'].apply(lambda x : len(re.compile(r\"#(\\w+)\").findall(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_mentions'] = df['text'].apply(lambda x : len(re.compile(r\"@(\\w+)\").findall(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_length</th>\n",
       "      <th>num_hashtags</th>\n",
       "      <th>num_mentions</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>113.947919</td>\n",
       "      <td>0.233384</td>\n",
       "      <td>1.111244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>87.359471</td>\n",
       "      <td>0.212004</td>\n",
       "      <td>1.167473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>86.082945</td>\n",
       "      <td>0.297503</td>\n",
       "      <td>1.138383</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   tweet_length  num_hashtags  num_mentions\n",
       "airline_sentiment                                          \n",
       "negative             113.947919      0.233384      1.111244\n",
       "neutral               87.359471      0.212004      1.167473\n",
       "positive              86.082945      0.297503      1.138383"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if there is significant difference in the length of tweet across different sentiments\n",
    "# Check if there is significant difference in the number of hashtags across different sentiments\n",
    "# Check if there is significant difference in the number of mentions across different sentiments\n",
    "\n",
    "mean_df = df.groupby('airline_sentiment').mean()\n",
    "mean_df[['tweet_length','num_hashtags', 'num_mentions']]\n",
    "\n",
    "# Result - not siginificant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dictionary of all tweets\n",
    "tweet_dictionary = {}\n",
    "i = 0\n",
    "for line in dataset:\n",
    "        tweet_dictionary[i] = line.lower()\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_links(text):\n",
    "    link_regex    = re.compile('((https?):((//)|(\\\\\\\\))+([\\w\\d:#@%/;$()~_?\\+-=\\\\\\.&](#!)?)*)', re.DOTALL)\n",
    "    links         = re.findall(link_regex, text)\n",
    "    for link in links:\n",
    "        text = text.replace(link[0], ', ')    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(tweet_dictionary)):\n",
    "    tweet_dictionary[i]=strip_links(tweet_dictionary[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_mentions_hashtags(text):\n",
    "    entity_prefixes = ['@','#']\n",
    "    for separator in  string.punctuation:\n",
    "        if separator not in entity_prefixes :\n",
    "            text = text.replace(separator,' ')\n",
    "    words = []\n",
    "    for word in text.split():\n",
    "        word = word.strip()\n",
    "        if word:\n",
    "            if word[0] not in entity_prefixes:\n",
    "                words.append(word)\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(tweet_dictionary)):\n",
    "    tweet_dictionary[i]=strip_mentions_hashtags(tweet_dictionary[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(df)):\n",
    "    tweet_dictionary[i] = tweet_dictionary[i].replace('RT', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code source is http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "\n",
    "import re\n",
    "cList = {\n",
    "  \"ain't\": \"am not\",\n",
    "  \"aren't\": \"are not\",\n",
    "  \"can't\": \"cannot\",\n",
    "  \"can't've\": \"cannot have\",\n",
    "  \"'cause\": \"because\",\n",
    "  \"could've\": \"could have\",\n",
    "  \"couldn't\": \"could not\",\n",
    "  \"couldn't've\": \"could not have\",\n",
    "  \"didn't\": \"did not\",\n",
    "  \"doesn't\": \"does not\",\n",
    "  \"don't\": \"do not\",\n",
    "  \"hadn't\": \"had not\",\n",
    "  \"hadn't've\": \"had not have\",\n",
    "  \"hasn't\": \"has not\",\n",
    "  \"haven't\": \"have not\",\n",
    "  \"he'd\": \"he would\",\n",
    "  \"he'd've\": \"he would have\",\n",
    "  \"he'll\": \"he will\",\n",
    "  \"he'll've\": \"he will have\",\n",
    "  \"he's\": \"he is\",\n",
    "  \"how'd\": \"how did\",\n",
    "  \"how'd'y\": \"how do you\",\n",
    "  \"how'll\": \"how will\",\n",
    "  \"how's\": \"how is\",\n",
    "  \"I'd\": \"I would\",\n",
    "  \"I'd've\": \"I would have\",\n",
    "  \"I'll\": \"I will\",\n",
    "  \"I'll've\": \"I will have\",\n",
    "  \"I'm\": \"I am\",\n",
    "  \"I've\": \"I have\",\n",
    "  \"isn't\": \"is not\",\n",
    "  \"it'd\": \"it had\",\n",
    "  \"it'd've\": \"it would have\",\n",
    "  \"it'll\": \"it will\",\n",
    "  \"it'll've\": \"it will have\",\n",
    "  \"it's\": \"it is\",\n",
    "  \"let's\": \"let us\",\n",
    "  \"ma'am\": \"madam\",\n",
    "  \"mayn't\": \"may not\",\n",
    "  \"might've\": \"might have\",\n",
    "  \"mightn't\": \"might not\",\n",
    "  \"mightn't've\": \"might not have\",\n",
    "  \"must've\": \"must have\",\n",
    "  \"mustn't\": \"must not\",\n",
    "  \"mustn't've\": \"must not have\",\n",
    "  \"needn't\": \"need not\",\n",
    "  \"needn't've\": \"need not have\",\n",
    "  \"o'clock\": \"of the clock\",\n",
    "  \"oughtn't\": \"ought not\",\n",
    "  \"oughtn't've\": \"ought not have\",\n",
    "  \"shan't\": \"shall not\",\n",
    "  \"sha'n't\": \"shall not\",\n",
    "  \"shan't've\": \"shall not have\",\n",
    "  \"she'd\": \"she would\",\n",
    "  \"she'd've\": \"she would have\",\n",
    "  \"she'll\": \"she will\",\n",
    "  \"she'll've\": \"she will have\",\n",
    "  \"she's\": \"she is\",\n",
    "  \"should've\": \"should have\",\n",
    "  \"shouldn't\": \"should not\",\n",
    "  \"shouldn't've\": \"should not have\",\n",
    "  \"so've\": \"so have\",\n",
    "  \"so's\": \"so is\",\n",
    "  \"that'd\": \"that would\",\n",
    "  \"that'd've\": \"that would have\",\n",
    "  \"that's\": \"that is\",\n",
    "  \"there'd\": \"there had\",\n",
    "  \"there'd've\": \"there would have\",\n",
    "  \"there's\": \"there is\",\n",
    "  \"they'd\": \"they would\",\n",
    "  \"they'd've\": \"they would have\",\n",
    "  \"they'll\": \"they will\",\n",
    "  \"they'll've\": \"they will have\",\n",
    "  \"they're\": \"they are\",\n",
    "  \"they've\": \"they have\",\n",
    "  \"to've\": \"to have\",\n",
    "  \"wasn't\": \"was not\",\n",
    "  \"we'd\": \"we had\",\n",
    "  \"we'd've\": \"we would have\",\n",
    "  \"we'll\": \"we will\",\n",
    "  \"we'll've\": \"we will have\",\n",
    "  \"we're\": \"we are\",\n",
    "  \"we've\": \"we have\",\n",
    "  \"weren't\": \"were not\",\n",
    "  \"what'll\": \"what will\",\n",
    "  \"what'll've\": \"what will have\",\n",
    "  \"what're\": \"what are\",\n",
    "  \"what's\": \"what is\",\n",
    "  \"what've\": \"what have\",\n",
    "  \"when's\": \"when is\",\n",
    "  \"when've\": \"when have\",\n",
    "  \"where'd\": \"where did\",\n",
    "  \"where's\": \"where is\",\n",
    "  \"where've\": \"where have\",\n",
    "  \"who'll\": \"who will\",\n",
    "  \"who'll've\": \"who will have\",\n",
    "  \"who's\": \"who is\",\n",
    "  \"who've\": \"who have\",\n",
    "  \"why's\": \"why is\",\n",
    "  \"why've\": \"why have\",\n",
    "  \"will've\": \"will have\",\n",
    "  \"won't\": \"will not\",\n",
    "  \"won't've\": \"will not have\",\n",
    "  \"would've\": \"would have\",\n",
    "  \"wouldn't\": \"would not\",\n",
    "  \"wouldn't've\": \"would not have\",\n",
    "  \"y'all\": \"you all\",\n",
    "  \"y'alls\": \"you alls\",\n",
    "  \"y'all'd\": \"you all would\",\n",
    "  \"y'all'd've\": \"you all would have\",\n",
    "  \"y'all're\": \"you all are\",\n",
    "  \"y'all've\": \"you all have\",\n",
    "  \"you'd\": \"you had\",\n",
    "  \"you'd've\": \"you would have\",\n",
    "  \"you'll\": \"youyou will\",\n",
    "  \"you'll've\": \"you will have\",\n",
    "  \"you're\": \"you are\",\n",
    "  \"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "c_re = re.compile('(%s)' % '|'.join(cList.keys()))\n",
    "\n",
    "def expandContractions(text, c_re=c_re):\n",
    "    def replace(match):\n",
    "        return cList[match.group(0)]\n",
    "    text = c_re.sub(replace, text.lower())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(tweet_dictionary)):\n",
    "    tweet_dictionary[i]=expandContractions(tweet_dictionary[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(tweet_dictionary)):\n",
    "    tweet_dictionary[i]=remove_special_characters(tweet_dictionary[i], \n",
    "                          remove_digits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(tweet_dictionary)):\n",
    "    tweet_dictionary[i]=remove_stopwords(tweet_dictionary[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_stemmer(text):\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(tweet_dictionary)):\n",
    "#     tweet_dictionary[i]=simple_stemmer(tweet_dictionary[i])\n",
    "    tweet_dictionary[i]=lemmatize_text(tweet_dictionary[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = tweet_dictionary.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove words which are shorter than 3 characters. \n",
    "# This is for removing words with less meaning\n",
    "df['text'] = df['text'].apply(lambda x : re.sub(r'\\b\\w{1,3}\\b', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TfIdfVectorizer:\n",
    "# Remove words existing in less than min_df documents and remove words existing in more than max_df percentage of documents\n",
    "vectorizer = TfidfVectorizer(min_df=15, max_df=0.6, ngram_range=(1,3))\n",
    "temp_df = vectorizer.fit_transform(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = pd.DataFrame(temp_df.toarray(), columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>tweet_length</th>\n",
       "      <th>num_hashtags</th>\n",
       "      <th>num_mentions</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>5.692602e+17</td>\n",
       "      <td>0.933365</td>\n",
       "      <td>0.731769</td>\n",
       "      <td>0.093375</td>\n",
       "      <td>113.947919</td>\n",
       "      <td>0.233384</td>\n",
       "      <td>1.111244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>5.691841e+17</td>\n",
       "      <td>0.823303</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.060987</td>\n",
       "      <td>87.359471</td>\n",
       "      <td>0.212004</td>\n",
       "      <td>1.167473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>5.691006e+17</td>\n",
       "      <td>0.872039</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.069403</td>\n",
       "      <td>86.082945</td>\n",
       "      <td>0.297503</td>\n",
       "      <td>1.138383</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       tweet_id  airline_sentiment_confidence  \\\n",
       "airline_sentiment                                               \n",
       "negative           5.692602e+17                      0.933365   \n",
       "neutral            5.691841e+17                      0.823303   \n",
       "positive           5.691006e+17                      0.872039   \n",
       "\n",
       "                   negativereason_confidence  retweet_count  tweet_length  \\\n",
       "airline_sentiment                                                           \n",
       "negative                            0.731769       0.093375    113.947919   \n",
       "neutral                             0.000000       0.060987     87.359471   \n",
       "positive                            0.000000       0.069403     86.082945   \n",
       "\n",
       "                   num_hashtags  num_mentions  \n",
       "airline_sentiment                              \n",
       "negative               0.233384      1.111244  \n",
       "neutral                0.212004      1.167473  \n",
       "positive               0.297503      1.138383  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('airline_sentiment').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['airline_sentiment','retweet_count', 'airline']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "airline_one_hot = pd.get_dummies(df['airline'])\n",
    "df = df.drop('airline',axis=1)\n",
    "df = df.join(airline_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.concat([df, text_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>American</th>\n",
       "      <th>Delta</th>\n",
       "      <th>Southwest</th>\n",
       "      <th>US Airways</th>\n",
       "      <th>United</th>\n",
       "      <th>Virgin America</th>\n",
       "      <th>able</th>\n",
       "      <th>absolute</th>\n",
       "      <th>...</th>\n",
       "      <th>would like</th>\n",
       "      <th>would love</th>\n",
       "      <th>would nice</th>\n",
       "      <th>write</th>\n",
       "      <th>wrong</th>\n",
       "      <th>yeah</th>\n",
       "      <th>year</th>\n",
       "      <th>yell</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>zero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1202 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment  retweet_count  American  Delta  Southwest  US Airways  \\\n",
       "0           neutral              0         0      0          0           0   \n",
       "1          positive              0         0      0          0           0   \n",
       "2           neutral              0         0      0          0           0   \n",
       "3          negative              0         0      0          0           0   \n",
       "4          negative              0         0      0          0           0   \n",
       "\n",
       "   United  Virgin America  able  absolute  ...   would like  would love  \\\n",
       "0       0               1   0.0       0.0  ...          0.0         0.0   \n",
       "1       0               1   0.0       0.0  ...          0.0         0.0   \n",
       "2       0               1   0.0       0.0  ...          0.0         0.0   \n",
       "3       0               1   0.0       0.0  ...          0.0         0.0   \n",
       "4       0               1   0.0       0.0  ...          0.0         0.0   \n",
       "\n",
       "   would nice  write  wrong  yeah  year  yell  yesterday  zero  \n",
       "0         0.0    0.0    0.0   0.0   0.0   0.0        0.0   0.0  \n",
       "1         0.0    0.0    0.0   0.0   0.0   0.0        0.0   0.0  \n",
       "2         0.0    0.0    0.0   0.0   0.0   0.0        0.0   0.0  \n",
       "3         0.0    0.0    0.0   0.0   0.0   0.0        0.0   0.0  \n",
       "4         0.0    0.0    0.0   0.0   0.0   0.0        0.0   0.0  \n",
       "\n",
       "[5 rows x 1202 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "df_features = final_df.iloc[:, 1:].values\n",
    "df_target = final_df.iloc[:,0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier()\n",
    "# K-fold construction\n",
    "folds = 10\n",
    "kf = model_selection.KFold(n_splits=folds, shuffle=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Fold 1\n",
      "\tTRAIN: 13176 TEST: 1464\n",
      "\tPrediction:  ['positive' 'neutral' 'negative' ..., 'positive' 'negative' 'negative']\n",
      "\tCorrect:     ['positive' 'negative' 'negative' ..., 'neutral' 'negative' 'neutral']\n",
      "\tAccuracy: 0.707650273224\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.80      0.81       933\n",
      "     neutral       0.47      0.54      0.50       321\n",
      "    positive       0.61      0.58      0.59       210\n",
      "\n",
      "   micro avg       0.71      0.71      0.71      1464\n",
      "   macro avg       0.64      0.64      0.64      1464\n",
      "weighted avg       0.72      0.71      0.71      1464\n",
      "\n",
      "Starting Fold 2\n",
      "\tTRAIN: 13176 TEST: 1464\n",
      "\tPrediction:  ['positive' 'neutral' 'neutral' ..., 'negative' 'negative' 'negative']\n",
      "\tCorrect:     ['negative' 'positive' 'positive' ..., 'positive' 'negative' 'negative']\n",
      "\tAccuracy: 0.677595628415\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.77      0.79       933\n",
      "     neutral       0.44      0.51      0.47       314\n",
      "    positive       0.53      0.55      0.54       217\n",
      "\n",
      "   micro avg       0.68      0.68      0.68      1464\n",
      "   macro avg       0.60      0.61      0.60      1464\n",
      "weighted avg       0.69      0.68      0.68      1464\n",
      "\n",
      "Starting Fold 3\n",
      "\tTRAIN: 13176 TEST: 1464\n",
      "\tPrediction:  ['neutral' 'positive' 'neutral' ..., 'negative' 'positive' 'negative']\n",
      "\tCorrect:     ['positive' 'positive' 'positive' ..., 'negative' 'negative' 'negative']\n",
      "\tAccuracy: 0.695355191257\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.81      0.80       892\n",
      "     neutral       0.50      0.52      0.51       321\n",
      "    positive       0.58      0.51      0.55       251\n",
      "\n",
      "   micro avg       0.70      0.70      0.70      1464\n",
      "   macro avg       0.62      0.61      0.62      1464\n",
      "weighted avg       0.69      0.70      0.69      1464\n",
      "\n",
      "Starting Fold 4\n",
      "\tTRAIN: 13176 TEST: 1464\n",
      "\tPrediction:  ['positive' 'positive' 'neutral' ..., 'negative' 'negative' 'negative']\n",
      "\tCorrect:     ['neutral' 'positive' 'positive' ..., 'negative' 'negative' 'negative']\n",
      "\tAccuracy: 0.689890710383\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.81      0.80       908\n",
      "     neutral       0.48      0.47      0.47       320\n",
      "    positive       0.58      0.54      0.56       236\n",
      "\n",
      "   micro avg       0.69      0.69      0.69      1464\n",
      "   macro avg       0.61      0.60      0.61      1464\n",
      "weighted avg       0.69      0.69      0.69      1464\n",
      "\n",
      "Starting Fold 5\n",
      "\tTRAIN: 13176 TEST: 1464\n",
      "\tPrediction:  ['neutral' 'negative' 'neutral' ..., 'positive' 'positive' 'negative']\n",
      "\tCorrect:     ['negative' 'negative' 'positive' ..., 'positive' 'positive' 'neutral']\n",
      "\tAccuracy: 0.685792349727\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.78      0.80       906\n",
      "     neutral       0.44      0.49      0.47       306\n",
      "    positive       0.58      0.57      0.58       252\n",
      "\n",
      "   micro avg       0.69      0.69      0.69      1464\n",
      "   macro avg       0.61      0.61      0.61      1464\n",
      "weighted avg       0.69      0.69      0.69      1464\n",
      "\n",
      "Starting Fold 6\n",
      "\tTRAIN: 13176 TEST: 1464\n",
      "\tPrediction:  ['negative' 'neutral' 'positive' ..., 'neutral' 'negative' 'neutral']\n",
      "\tCorrect:     ['negative' 'neutral' 'positive' ..., 'negative' 'negative' 'positive']\n",
      "\tAccuracy: 0.679644808743\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.79      0.79       921\n",
      "     neutral       0.42      0.46      0.44       298\n",
      "    positive       0.59      0.54      0.56       245\n",
      "\n",
      "   micro avg       0.68      0.68      0.68      1464\n",
      "   macro avg       0.60      0.60      0.60      1464\n",
      "weighted avg       0.69      0.68      0.68      1464\n",
      "\n",
      "Starting Fold 7\n",
      "\tTRAIN: 13176 TEST: 1464\n",
      "\tPrediction:  ['positive' 'negative' 'negative' ..., 'negative' 'negative' 'negative']\n",
      "\tCorrect:     ['neutral' 'negative' 'neutral' ..., 'negative' 'negative' 'neutral']\n",
      "\tAccuracy: 0.683743169399\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.77      0.79       945\n",
      "     neutral       0.44      0.51      0.47       295\n",
      "    positive       0.56      0.57      0.56       224\n",
      "\n",
      "   micro avg       0.68      0.68      0.68      1464\n",
      "   macro avg       0.60      0.61      0.61      1464\n",
      "weighted avg       0.70      0.68      0.69      1464\n",
      "\n",
      "Starting Fold 8\n",
      "\tTRAIN: 13176 TEST: 1464\n",
      "\tPrediction:  ['neutral' 'neutral' 'positive' ..., 'negative' 'negative' 'negative']\n",
      "\tCorrect:     ['positive' 'neutral' 'positive' ..., 'negative' 'neutral' 'negative']\n",
      "\tAccuracy: 0.680327868852\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.80      0.79       905\n",
      "     neutral       0.43      0.47      0.45       311\n",
      "    positive       0.63      0.53      0.57       248\n",
      "\n",
      "   micro avg       0.68      0.68      0.68      1464\n",
      "   macro avg       0.61      0.60      0.60      1464\n",
      "weighted avg       0.68      0.68      0.68      1464\n",
      "\n",
      "Starting Fold 9\n",
      "\tTRAIN: 13176 TEST: 1464\n",
      "\tPrediction:  ['neutral' 'negative' 'neutral' ..., 'negative' 'negative' 'negative']\n",
      "\tCorrect:     ['neutral' 'neutral' 'negative' ..., 'negative' 'negative' 'negative']\n",
      "\tAccuracy: 0.67349726776\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.78      0.79      0.78       922\n",
      "     neutral       0.44      0.45      0.44       302\n",
      "    positive       0.56      0.53      0.55       240\n",
      "\n",
      "   micro avg       0.67      0.67      0.67      1464\n",
      "   macro avg       0.59      0.59      0.59      1464\n",
      "weighted avg       0.67      0.67      0.67      1464\n",
      "\n",
      "Starting Fold 10\n",
      "\tTRAIN: 13176 TEST: 1464\n",
      "\tPrediction:  ['neutral' 'positive' 'negative' ..., 'positive' 'positive' 'negative']\n",
      "\tCorrect:     ['neutral' 'positive' 'negative' ..., 'positive' 'positive' 'negative']\n",
      "\tAccuracy: 0.69262295082\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.78      0.79       913\n",
      "     neutral       0.47      0.53      0.50       311\n",
      "    positive       0.60      0.55      0.58       240\n",
      "\n",
      "   micro avg       0.69      0.69      0.69      1464\n",
      "   macro avg       0.62      0.62      0.62      1464\n",
      "weighted avg       0.70      0.69      0.70      1464\n",
      "\n",
      "Average Accuracy: 0.687\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.79      0.79      9178\n",
      "     neutral       0.45      0.49      0.47      3099\n",
      "    positive       0.58      0.55      0.56      2363\n",
      "\n",
      "   micro avg       0.69      0.69      0.69     14640\n",
      "   macro avg       0.61      0.61      0.61     14640\n",
      "weighted avg       0.69      0.69      0.69     14640\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# K-fold cross validation and performance evaluation\n",
    "foldid = 0\n",
    "totacc = 0.\n",
    "ytlog = []\n",
    "yplog = []\n",
    "for train_index, test_index in kf.split(df_features):\n",
    "    foldid += 1\n",
    "    print(\"Starting Fold %d\" % foldid)\n",
    "    print(\"\\tTRAIN:\", len(train_index), \"TEST:\", len(test_index))\n",
    "    X_train, X_test = df_features[train_index], df_features[test_index]\n",
    "    y_train, y_test = df_target[train_index], df_target[test_index]\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    acc = accuracy_score(y_pred, y_test)\n",
    "    totacc += acc\n",
    "    ytlog += list(y_test)\n",
    "    yplog += list(y_pred)\n",
    "    \n",
    "    print('\\tPrediction: ', y_pred)\n",
    "    print('\\tCorrect:    ', y_test)\n",
    "    print('\\tAccuracy:', acc)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"Average Accuracy: %0.3f\" % (totacc / folds,))\n",
    "print(classification_report(ytlog, yplog))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Random Forest Classifier\n",
    "clf = RandomForestClassifier()\n",
    "# K-fold construction\n",
    "folds = 5\n",
    "kf = model_selection.KFold(n_splits=folds, shuffle=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Fold 1\n",
      "\tTRAIN: 11712 TEST: 2928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gemin\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tPrediction:  ['positive' 'neutral' 'neutral' ..., 'negative' 'negative' 'negative']\n",
      "\tCorrect:     ['positive' 'negative' 'positive' ..., 'negative' 'neutral' 'neutral']\n",
      "\tAccuracy: 0.731215846995\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.78      0.89      0.83      1825\n",
      "     neutral       0.54      0.42      0.47       618\n",
      "    positive       0.72      0.54      0.62       485\n",
      "\n",
      "   micro avg       0.73      0.73      0.73      2928\n",
      "   macro avg       0.68      0.62      0.64      2928\n",
      "weighted avg       0.72      0.73      0.72      2928\n",
      "\n",
      "Starting Fold 2\n",
      "\tTRAIN: 11712 TEST: 2928\n",
      "\tPrediction:  ['neutral' 'negative' 'positive' ..., 'negative' 'positive' 'negative']\n",
      "\tCorrect:     ['positive' 'neutral' 'positive' ..., 'positive' 'positive' 'neutral']\n",
      "\tAccuracy: 0.727459016393\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.78      0.88      0.83      1806\n",
      "     neutral       0.54      0.45      0.49       640\n",
      "    positive       0.69      0.51      0.59       482\n",
      "\n",
      "   micro avg       0.73      0.73      0.73      2928\n",
      "   macro avg       0.67      0.61      0.64      2928\n",
      "weighted avg       0.71      0.73      0.72      2928\n",
      "\n",
      "Starting Fold 3\n",
      "\tTRAIN: 11712 TEST: 2928\n",
      "\tPrediction:  ['neutral' 'neutral' 'negative' ..., 'neutral' 'negative' 'negative']\n",
      "\tCorrect:     ['neutral' 'negative' 'positive' ..., 'positive' 'negative' 'negative']\n",
      "\tAccuracy: 0.734972677596\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.89      0.84      1849\n",
      "     neutral       0.55      0.44      0.49       638\n",
      "    positive       0.67      0.53      0.59       441\n",
      "\n",
      "   micro avg       0.73      0.73      0.73      2928\n",
      "   macro avg       0.67      0.62      0.64      2928\n",
      "weighted avg       0.72      0.73      0.72      2928\n",
      "\n",
      "Starting Fold 4\n",
      "\tTRAIN: 11712 TEST: 2928\n",
      "\tPrediction:  ['neutral' 'neutral' 'negative' ..., 'negative' 'positive' 'negative']\n",
      "\tCorrect:     ['negative' 'neutral' 'positive' ..., 'negative' 'positive' 'negative']\n",
      "\tAccuracy: 0.728483606557\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.88      0.83      1855\n",
      "     neutral       0.51      0.41      0.46       599\n",
      "    positive       0.66      0.54      0.59       474\n",
      "\n",
      "   micro avg       0.73      0.73      0.73      2928\n",
      "   macro avg       0.66      0.61      0.63      2928\n",
      "weighted avg       0.71      0.73      0.72      2928\n",
      "\n",
      "Starting Fold 5\n",
      "\tTRAIN: 11712 TEST: 2928\n",
      "\tPrediction:  ['negative' 'negative' 'positive' ..., 'neutral' 'neutral' 'negative']\n",
      "\tCorrect:     ['negative' 'neutral' 'positive' ..., 'positive' 'negative' 'negative']\n",
      "\tAccuracy: 0.738046448087\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.87      0.84      1843\n",
      "     neutral       0.53      0.47      0.50       604\n",
      "    positive       0.69      0.55      0.61       481\n",
      "\n",
      "   micro avg       0.74      0.74      0.74      2928\n",
      "   macro avg       0.67      0.63      0.65      2928\n",
      "weighted avg       0.73      0.74      0.73      2928\n",
      "\n",
      "Average Accuracy: 0.732\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.88      0.83      9178\n",
      "     neutral       0.53      0.44      0.48      3099\n",
      "    positive       0.69      0.53      0.60      2363\n",
      "\n",
      "   micro avg       0.73      0.73      0.73     14640\n",
      "   macro avg       0.67      0.62      0.64     14640\n",
      "weighted avg       0.72      0.73      0.72     14640\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# K-fold cross validation and performance evaluation\n",
    "foldid = 0\n",
    "totacc = 0.\n",
    "ytlog = []\n",
    "yplog = []\n",
    "for train_index, test_index in kf.split(df_features):\n",
    "#     print(train_index)\n",
    "    foldid += 1\n",
    "    print(\"Starting Fold %d\" % foldid)\n",
    "    print(\"\\tTRAIN:\", len(train_index), \"TEST:\", len(test_index))\n",
    "    X_train, X_test = df_features[train_index], df_features[test_index]\n",
    "    y_train, y_test = df_target[train_index], df_target[test_index]\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    acc = accuracy_score(y_pred, y_test)\n",
    "    totacc += acc\n",
    "    ytlog += list(y_test)\n",
    "    yplog += list(y_pred)\n",
    "    \n",
    "    print('\\tPrediction: ', y_pred)\n",
    "    print('\\tCorrect:    ', y_test)\n",
    "    print('\\tAccuracy:', acc)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"Average Accuracy: %0.3f\" % (totacc / folds,))\n",
    "print(classification_report(ytlog, yplog))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
